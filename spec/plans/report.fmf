summary: Report test results
story:
    As a tester I want to have a nice overview of results once
    the testing if finished.
description:
    Report test results according to user preferences.

/display:
    summary: Show results in the terminal window
    story:
        As a tester I want to see test results in the plain text
        form in my shell session.
    description:
        Test results will be displayed as part of the command line
        tool output directly in the terminal. Allows to select the
        desired level of verbosity
    example: |
        tmt run -l report        # overall summary only
        tmt run -l report -v     # individual test results
        tmt run -l report -vv    # show full paths to logs
        tmt run -l report -vvv   # provide complete test output
    link:
      - implemented-by: /tmt/steps/report/display.py

/html:
    summary: Generate a web page with test results
    story:
        As a tester I want to review results in a nicely arranged
        web page with links to detailed test output.
    description:
        Create a local ``html`` file with test results arranged in
        a table. Optionally open the page in the default browser.
    example: |
        # Enable html report from the command line
        tmt run --all report --how html
        tmt run --all report --how html --open
        tmt run -l report -h html -o

        # Use html as the default report for given plan
        report:
            how: html
            open: true
    link:
      - implemented-by: /tmt/steps/report/html.py

/junit:
    summary: Generate a JUnit report file
    story:
        As a tester I want to review results in a JUnit xml file.
    description:
        Create a JUnit file ``junit.xml`` with test results.
    example: |
        # Enable junit report from the command line
        tmt run --all report --how junit
        tmt run --all report --how junit --file test.xml

        # Use junit as the default report for given plan
        report:
            how: junit
            file: test.xml
    link:
        - implemented-by: /tmt/steps/report/junit.py

/polarion:
    summary: Generate a xUnit file and export it into Polarion
    story:
        As a tester I want to review tests in Polarion
        and have all results linked to existing test cases there.
    description:
        Create a xUnit file ``xunit.xml`` with test results
        and Polarion properties so the xUnit can then be
        exported into Polarion.
    example: |
        # Enable polarion report from the command line
        tmt run --all report --how polarion --project-id TMT
        tmt run --all report --how polarion --project-id TMT --no-upload --file test.xml

        # Use polarion as the default report for given plan
        report:
            how: polarion
            file: test.xml
            project-id: TMT
            testrun-title: tests_that_pass
    link:
        - implemented-by: /tmt/steps/report/polarion.py

/file:
    description: |

        Save the report into a ``report.yaml`` file with the
        following format::

            result: OVERALL_RESULT
            plans:
                /plan/one:
                    result: PLAN_OUTCOME
                    tests:
                        /test/one:
                            result: TEST_OUTCOME
                            log: LOG_PATH

                        /test/two:
                            result: TEST_OUTCOME
                            log:
                                - LOG_PATH
                                - LOG_PATH
                                - LOG_PATH
                /plan/two:
                    result: PLAN_OUTCOME
                        /test/one:
                            result: TEST_OUTCOME
                            log: LOG_PATH

        Where ``OVERALL_RESULT`` is the overall result of all plan
        results. It is counted the same way as ``PLAN_OUTCOME``.

        Where ``TEST_OUTCOME`` is the same as ``OUTCOME`` in
        the `execute`_ step definition:

        pass
            Test execution successfully finished and passed.
        info
            Test finished but only produced an informational
            message. Represents a soft pass, used for skipped
            tests and for tests with the :ref:`/spec/tests/result`
            attribute set to *ignore*. Automation must treat
            this as a passed test.
        warn
            A problem appeared during test execution which does
            not affect test results but might be worth checking
            and fixing. For example test cleanup phase failed
            or a check failure for a passing test. Automation
            must treat this as a failed test.
        error
            Undefined problem encountered during test execution.
            Human inspection is needed to investigate whether it
            was a test bug, infrastructure error or a real test
            failure. Automation must treat it as a failed test.
        fail
            Test execution successfully finished and failed.

        Note the priority of test results is as written above,
        with ``info`` having the lowest priority and ``error`` has
        the highest. This is important for ``PLAN_OUTCOME``.

        Where ``PLAN_OUTCOME`` is the overall result or all test
        results for the plan run. It has the same values as
        ``TEST_OUTCOME``. Plan result is counted according to the
        priority of the test outcome values. For example:

            * if the test results are info, passed, passed - the
              plan result will be passed
            * if the test results are info, passed, failed - the
              plan result will be failed
            * if the test results are failed, error, passed - the
              plan result will be error

        Where ``LOG_PATH`` is the test log output path, relative
        to the execute step plan run directory.  The log can be a
        single log path or a list of log paths, in case the test
        has produced more log files.
